==============================================
      HowTo create a new DBpedia release
==============================================

Author: Max Jakob (max.jakob@fu-berlin.de)
Date: 2011-01-21

This file describes the main steps necessary for creating a new DBpedia
release. It might not be complete. Please also consult with the others!


 1. improve extraction framework by implementing ToDos
    - check also the bug tracker on SourceForge (http://sourceforge.net/tracker/?group_id=190976&atid=935520)

 2. clean up the ontology and improve the (mappings on the wiki http://mappings.dbpedia.org/)

 3. download Wikipedia XML and SQL dumps using dump/Download.scala
    - specify minimum number of articles a Wikipedia must have in order to be included
    - make sure that namespace recognition works for all downloaded languages (check core/wikiparser.impl.wikipedia.Namespaces.scala)

 4. distribute the dumps onto multiple computers
    - every computer must have the commons dump

 5. for AbstractExtractor: insert Wikipedia dumps into a local MySQL database using the script import.sh
    - adjust the LocalSettings.php of mw-modified: specify username+password for the database and the database prefix


 6. extract datasets for all languages
    - run the MappingExtractor for all languages that have mappings on the wiki
    - consider starting the abstract extraction earlier (e.g. before cleaning up the mappings) because it takes quite long


 7. test load part of http://wiki.dbpedia.org/DatasetsLoaded into an RDF store
    - especially new extended datasets (YAGO, Feebase etc.) to see whether there are syntax errors
    - browse through the data to make sure everything is there (e.g. using Pubby)

 8. export OWL: http://mappings.dbpedia.org/server/ontology/export

 9. compress all dumps to .bz2 files using dump/Compress.scala
    - compress OWL manually

10. run SVN/related_apps/downloadpagecreator/downloadpagecreator.php (configure variables at top of script beforehand)
    - paste the created code in http://wiki.dbpedia.org/Downloads
      - split the download page into multiple wiki pages. If the wiki code is too long for a sinlge page it gets truncated and produces rubbish web pages

11. compress all already compressed files into one archive ''all_languages.tar''

12. send:
    - all data specified on http://wiki.dbpedia.org/DatasetsLoaded zipped to OpenLink (ask Chris for contacts)
    - all_languages.tar to Jens from Uni Leipzig

13. update http://wiki.dbpedia.org/
    - get some statistics for: classes, triples, mappings, ...

14. write the release notes

15. check if all example links on http://wiki.dbpedia.org/ are still working with the new end point

16. update Wikipedia article

